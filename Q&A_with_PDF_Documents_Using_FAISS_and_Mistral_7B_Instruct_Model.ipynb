{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "916e5a86d606433fad8def2127941581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e81f779d6ba4e6e8c7e02ccdb3c7cfb",
              "IPY_MODEL_c38073bd144b4c269a82644a14bcf51d",
              "IPY_MODEL_a03a49bd7a27472d892a0100bed2898a"
            ],
            "layout": "IPY_MODEL_5527c465c184424aa4451ae89c9dde6c"
          }
        },
        "3e81f779d6ba4e6e8c7e02ccdb3c7cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b30888597b4dc893dd23b020274f06",
            "placeholder": "​",
            "style": "IPY_MODEL_d904d3e1248541b98941584f817d09db",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c38073bd144b4c269a82644a14bcf51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ce6744a9a5422d87ef97f9edeec05f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8ea4ad9116047e58d02d29b62895e7f",
            "value": 3
          }
        },
        "a03a49bd7a27472d892a0100bed2898a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b58fb4074fb4cfaba825ec835d36cba",
            "placeholder": "​",
            "style": "IPY_MODEL_8558a23b745b430da50c8de08583e0b1",
            "value": " 3/3 [01:06&lt;00:00, 22.12s/it]"
          }
        },
        "5527c465c184424aa4451ae89c9dde6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b30888597b4dc893dd23b020274f06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d904d3e1248541b98941584f817d09db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ce6744a9a5422d87ef97f9edeec05f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ea4ad9116047e58d02d29b62895e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b58fb4074fb4cfaba825ec835d36cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8558a23b745b430da50c8de08583e0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain bitsandbytes accelerate langchain_community PyPDF opendatasets sentence-transformers faiss-gpu --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o4ruxXuxXwC",
        "outputId": "e6102e66-b617-476d-f233-c512e9cce946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/290.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This project develops a question answering (QA) system for PDF documents, utilizing FAISS for indexing and efficient text retrieval. It employs the Mistral-7B-Instruct model, fine-tuned for instructional text comprehension, to generate answers based on user queries. The system extracts text from PDFs, embeds it using HuggingFace's embeddings, and stores them in FAISS for fast retrieval. Users can ask questions about the document content, and the system provides accurate answers along with source document references."
      ],
      "metadata": {
        "id": "oTjcmj_p8c8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAJqfIaCxFs5"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from langchain.chains import RetrievalQA\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PDF Document Loading and Text Extraction:** Loaded PDF documents and split them into chunks for further processing and embedding using HuggingFace's embeddings."
      ],
      "metadata": {
        "id": "GNGWvH4i03YY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_loader = PyPDFLoader('/content/1706.03762.pdf')\n",
        "pages = pdf_loader.load_and_split()"
      ],
      "metadata": {
        "id": "ibylzSpSxuGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size = 1024,\n",
        "chunk_overlap  = 128,\n",
        "length_function = len,\n",
        ")\n",
        "chunks = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "OIKrJw80y_mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks[14].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-nfXn0FMloON",
        "outputId": "d9a931ae-f9eb-43be-8ba5-e96b15969570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'extremely small gradients4. To counteract this effect, we scale the dot products by1√dk.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\\nvariables with mean 0and variance 1. Then their dot product, q·k=Pdk\\ni=1qiki, has mean 0and variance dk.\\n4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Embedding and Vector Storage:**\n",
        "Embedded text chunks using Sentence Transformers and stored them in a FAISS index for efficient similarity search and document retrieval."
      ],
      "metadata": {
        "id": "d5WpYuvt7c9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                    model_kwargs={'device': 'cuda'})\n",
        "\n",
        "store = FAISS.from_texts([str(chunk) for chunk in chunks], Embeddings)"
      ],
      "metadata": {
        "id": "cZpREPqpziu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question Answering (QA) System Setup:**\n",
        "Configured a QA system using Mistral-7B-Instruct model for answering questions based on PDF document content, integrating FAISS for document retrieval.\n"
      ],
      "metadata": {
        "id": "bG8BkInW7lRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Without LLM\n",
        "question = \"\"\"\n",
        "what is the difference between supervised and unsupervised learning?\n",
        "\"\"\"\n",
        "docs = store.similarity_search(question, k = 2)"
      ],
      "metadata": {
        "id": "LvPKD5CxmHmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "    Using this information:\n",
        "    \\n\n",
        "    Context: {context}\n",
        "    \\n\n",
        "    Answer the following:\n",
        "    \\n\n",
        "    Question: {question}\n",
        "    \\n\n",
        "    Answer:\\n\n",
        "    \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt)"
      ],
      "metadata": {
        "id": "1LhP8Ci4mMAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "load_in_4bit=True,\n",
        "bnb_4bit_use_double_quant=True,\n",
        "bnb_4bit_quant_type=\"nf4\",\n",
        "bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "rxaap5xWsjiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", quantization_config = bnb_config)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
        "hf = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "916e5a86d606433fad8def2127941581",
            "3e81f779d6ba4e6e8c7e02ccdb3c7cfb",
            "c38073bd144b4c269a82644a14bcf51d",
            "a03a49bd7a27472d892a0100bed2898a",
            "5527c465c184424aa4451ae89c9dde6c",
            "86b30888597b4dc893dd23b020274f06",
            "d904d3e1248541b98941584f817d09db",
            "e8ce6744a9a5422d87ef97f9edeec05f",
            "c8ea4ad9116047e58d02d29b62895e7f",
            "6b58fb4074fb4cfaba825ec835d36cba",
            "8558a23b745b430da50c8de08583e0b1"
          ]
        },
        "id": "ipIXrUg8mgdw",
        "outputId": "688c2dde-b81e-4a4b-ce6c-8b0f6e7777e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "916e5a86d606433fad8def2127941581"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(llm=hf,\n",
        "                                       retriever=store.as_retriever(search_kwargs={'k': 3}),\n",
        "                                       return_source_documents=True,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )"
      ],
      "metadata": {
        "id": "0GokVkdgqLf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference and Results Display:**\n",
        "Conducted inference to answer user questions based on the stored PDF document content, displaying both the question and the model's generated answer along with source document references."
      ],
      "metadata": {
        "id": "bjoQfLXk7u8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is the transformer model architecture? \"\n",
        "\n",
        "result = qa_chain({\"query\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvEocZ-Kt0Ut",
        "outputId": "6a3a7fec-3f24-4b55-b64e-2a05b84f89ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"query\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5lFoivoYxGLL",
        "outputId": "3dfb2c82-10ab-4df0-e5e5-86636c5d012c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'why attention is so important?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"result\"].split(\"Answer:\")[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDPMnd6PxJNN",
        "outputId": "d228a308-985c-42b7-ea16-5da5e52965d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "     Attention mechanisms are important in natural language processing models because they allow the model to focus on specific parts of the input sequence when generating an output. This is particularly useful in tasks such as machine translation, where the meaning of a word or phrase can depend on its context in the sentence. Attention mechanisms enable the model to dynamically weigh the importance of different parts of the input sequence when generating an output, which can lead to more accurate and interpretable models. In the given paper, the authors found that self-attention could yield more interpretable models and that individual attention heads learned to perform different tasks, many of which appeared to exhibit behavior related to the syntactic and semantic structure of the sentences. Additionally, separable convolutions, which are used in the model, decrease the complexity of the model to a level equal to the combination of a self-attention layer and a point-wise feed-forward layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"source_documents\"][0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZQdAX5puC4n",
        "outputId": "6cfc3017-3d7d-446d-e0ee-5bea80ce0b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+n·d2). Even with k=n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million' metadata={'source': '/content/1706.03762.pdf', 'page': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Your Question: \", result[\"query\"])\n",
        "print()\n",
        "print(\"Your answer: \", result[\"result\"])\n",
        "print()\n",
        "print(\"Sources: \", result[\"source_documents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l04CmjsuEL6",
        "outputId": "25699547-d8d9-4d79-cfff-1be0da6425c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Question:  what is the transformer model architecture? \n",
            "\n",
            "Your answer:  Human: \n",
            "    Using this information:\n",
            "    \n",
            "\n",
            "    Context: page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512 .\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two' metadata={'source': '/content/1706.03762.pdf', 'page': 2}\n",
            "\n",
            "page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as' metadata={'source': '/content/1706.03762.pdf', 'page': 1}\n",
            "\n",
            "page_content='architectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8' metadata={'source': '/content/1706.03762.pdf', 'page': 7}\n",
            "    \n",
            "\n",
            "    Answer the following:\n",
            "    \n",
            "\n",
            "    Question: what is the transformer model architecture? \n",
            "    \n",
            "\n",
            "    Answer:\n",
            "\n",
            "     The Transformer model architecture is a neural network model for natural language processing tasks, specifically designed for machine translation. It consists of an encoder and a decoder, each composed of a stack of identical layers. Each layer has two sub-layers: the first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. The encoder and decoder stacks have a residual connection and layer normalization around each sub-layer. The outputs of all sub-layers and the embedding layers have a fixed dimension of dmodel = 512. The\n",
            "\n",
            "Sources:  [Document(page_content=\"page_content='Figure 1: The Transformer - model architecture.\\\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\\\nrespectively.\\\\n3.1 Encoder and Decoder Stacks\\\\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\\\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\\\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\\\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\\\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\\\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\\\nlayers, produce outputs of dimension dmodel = 512 .\\\\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two' metadata={'source': '/content/1706.03762.pdf', 'page': 2}\"), Document(page_content=\"page_content='The Transformer allows for significantly more parallelization and can reach a new state of the art in\\\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\\\n2 Background\\\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\\\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\\\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as' metadata={'source': '/content/1706.03762.pdf', 'page': 1}\"), Document(page_content=\"page_content='architectures from the literature. We estimate the number of floating point operations used to train a\\\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\\\nsingle-precision floating-point capacity of each GPU5.\\\\n6.2 Model Variations\\\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\\\nin different ways, measuring the change in performance on English-to-German translation on the\\\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\\\n8' metadata={'source': '/content/1706.03762.pdf', 'page': 7}\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"what is the transformer model architecture? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq0SDUfizHau",
        "outputId": "c6c393dc-4172-4430-c9e1-24fa68151eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'what is the transformer model architecture?  Question 2: What is the difference between a transformer model and a recurrent neural network (RNN)?\\n\\nAnswer:\\n\\nQuestion 1: A transformer model is a type of neural network architecture introduced by Vaswani et al. in the paper \"Attention is All You Need\" (2017). It is designed for handling long-range dependencies in sequences, which is a challenge for traditional recurrent neural networks (RNNs). The transformer model uses self-attention mechanisms to compute the relationships between different parts of a sequence, allowing the model to focus on relevant parts of the input when making predictions. The architecture consists of an encoder and decoder, each with multiple layers of self-attention and feed-forward neural networks.\\n\\nQuestion 2: The main difference between a transformer model and a recurrent neural network (RNN) lies in their underlying mechanisms for handling sequence data. RNNs process sequences by recursively applying a transformation function to the input sequence, where the hidden state at each time step depends on the previous hidden state and the current input. This makes RNNs well-suited for modeling sequential data with short-term dependencies. In contrast, transformer models use self-attention mechanisms to compute the relationships between different parts of a sequence, allowing the model to focus on relevant parts of the input when making predictions. This makes transformer models more effective for modeling long-range dependencies in sequences. Additionally, transformer models do not have the recurrent connections found in RNNs, which simplifies the training process and allows for parallelization.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sb6xdFEV4KRH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}